# Game-Data-Analysis

## LeagueOfLegends 데이터 분석
### 목표
- 최근 30일간 유저 경기 데이터를 분석하여 유저의 이탈 및 잔존율, 신규 콘텐츠에 대한 접근율 등을 대시보드로 가시화하자.
### 요약
- Pipeline은 다음과 같이 설계했습니다.
  1. **Data-collect**
     - parqeut 형태로 주어진 날짜의 데이터를 수집한다. 
     - 이 때 summoner_id를 기준으로 쪼갠 shards 꼴로 저장하여 Resume=True인 경우나 데이터 업데이트/삭제가 이루어질 때 더 편리하게 접근할 수 있게 하였다.
     - 주어진 data recipe를 저장해뒀다가 Resume=True 인자가 주어지면 해당 recipe와 적재된 데이터 현황을 비교하고 필요한 데이터 리스트만 뽑을 수 있게 하였다.
     - 주로 메모리에서 작업하고 성공적으로 데이터 처리 작업이 끝났을 때만 로컬에 적재하게끔 설계하여 로컬의 데이터에서 데이터 무결성을 보장했다.
  2. **Data-upload**
     - 로컬에 *.parquet 파일로 적재된 shards를 Duckdb를 활용해 한 번에 읽고 분석에 활용하기 좋은 raw_data.db 라는 이름의 로컬 DWH를 만든다.
  3. **Data-analyze**
     - raw_data.db를 읽고 비즈니스 지표로 설정한 이탈율과 참여율을 분석하고 결과를 report.db의 각 table로 저장한다.
  4. **Dashboard-run**
     - 분석 결과를 저장한 report.db를 읽어 미리 설정한 시각화 자료를 그려주는 대시보드를 docker를 활용해 백그라운드에 띄운다.
- API Key가 하루에 한 번 사람이 직접 CAPCHA를 뚫고 업데이트해주어야 해서 완전 자동화는 하지 못하였습니다.
- 티어 분포를 고려한 비례층화추출 방법으로 샘플을 뽑았으며, 샘플 수가 부족한 상위 티어의 데이터를 증강한 후 분석 단계에서 가중치 보정을 적용해주었습니다.
- GPT 활용 부분은 다음과 같습니다.
    - **코드 질문** :  “yaml, .env 읽는 방법”
    - **개념 질문** : “가중치 보정 방법”
    - **이해 확인** : “~에 가중치 보정을 ~하게 하려고 해. 비판적으로 의견 내봐.”
